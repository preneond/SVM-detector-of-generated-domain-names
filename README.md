# SVM detector of generated domain names

## 1 Introduction

The domain name systems (DNS) are nowadays frequently misused by a certain type of malware.
For example, malware programs installed by attackers on a host computer try to use the DNS
as a mean to establish connection with the Command and Control server in order to become
a part of a botnet. The Command and Control server first registers a list of algorithmically
generated domain names. The malware on an infected computer then queries the domain names
generated by the same algorithm. The large number of generated domains prevents their manual
detection. Examples of legitimate domain names and those generated by malware:

```
LEGITIMATE GENERATED
---------------- ------------------
google.com atqgkfauhuaufm.com
facebook.com vopydum.com
youtube.com jgiugobavtyfsck.biz
yahoo.com dhjopxgdetn.com
baidu.com hhjjqjghfir.com
wikipedia.org ldivjfkfamhnzjvbabqylvsij.info
qq.com towngwvyjaebrtp.com
amazon.com yyryxlgbgxsy.biz
live.com mpwnnvqmxtnkv.ru
... ...
```
Our goal will be to design a classifier able to distinguish the generated domain names from
the legitimate ones automatically. We will learn such classifier from a training set of examples
by the Support Vector Machine algorithm. The training setT ={(x^1 ,y^1 ),...,(xm,ym)} ∈
(X × {+1,− 1 })mis composed of pairs of domain names and their labels. The input space
X= Σ∗contains all strings that can be generated from a finite alphabet Σ. The labely= +
will denote a generated malicious domain name whiley=−1 will stand for a legitimate domain.
We will represent the input strings by the String Sub-sequence Kernel (SSK) [3]. The SSK
kq: Σ∗×Σ∗→Rcomputes a dot product,kq(x,x′) =〈φ(x),φ(x′)〉, between two input strings
embedded to a feature space via the mapφ: Σ∗→R|Σ|
q

. The SSK compares two strings by
means of the substrings of lengthqthat may not be contiguous. The more substrings the input
strings have in common, the more similar they are. The kernel is constructed such that less
contiguous substrings contribute with less weight to the overall string similarity. The formal
definition of the SSK is described in more details below.
This computer lab has been motivated by the paper [2].


## 2 Support Vector Machines

In the primal formulation, the SVM algorithm learns parameters (w,b)∈(Rd,R) of the linear
classifier

```
h(x;w,b) = sign
```
#### (

```
〈w,φ(x)〉+b
```
#### )

#### , (1)

by solving the following convex program

(w∗,b∗) = argmin
w∈Rd,b∈R

#### (

#### 1

#### 2

```
‖w‖^2 +C
```
```
∑m
```
```
i=
```
```
ξi
```
#### )

s.t.
yi(〈w,φ(xi)〉+b) ≥ 1 −ξi, i∈{ 1 ,...,m},
ξi ≥ 0 , i∈{ 1 ,...,m},
(2)
whereC >0 is a constant which controls the trade-off between the empirical error and the
simplicity of the solution measured in terms of theL 2 -norm.
The Lagrange dual of (2) reads

```
α∗= argmax
α∈[0,C]m
```
```
(∑m
```
```
i=
```
```
αi−
```
#### 1

#### 2

```
∑m
```
```
i=
```
```
∑m
```
```
j=
```
```
〈φ(xi),φ(xj)〉αiαjyiyj
```
#### )

```
s.t.
```
```
∑m
```
```
i=
```
```
αiyi= 0, (3)
```
Having the dual solutionα∗, the primal solution is obtained by

```
w∗=
```
```
∑m
```
```
i=
```
```
α∗iyiφ(xi) and b∗=yi−〈w∗,φ(xi)〉 for any i∈ISVb , (4)
```
whereISVb ={i∈{ 1 ,...,m}| 0 < αi∗< C}. After substituting (4) to (1) we get

```
h(x;α∗,b∗) = sign
```
```
(∑m
```
```
i=
```
```
yiα∗i〈φ(xi),φ(x)〉+b∗
```
#### )

#### . (5)

It is seen from (3), (4) and (5) that learning and evaluation of the classifier requires the inputs
in terms of the dot products only. Therefore we can substitutek(xi,xj) for any appearance of
〈φ(xi),φ(xj)〉and use only the kernel functions that can be evaluated efficiently as we will see
in the next section.
In your implementation you can use any library providing solver for (3). The SVM library
should allow to use user-defined kernel matrix. You can also implement your own SVM solver
but it is not required. We recommend the libSVM library [1] which provides simple interfaces
for large number of programming languages including Matlab and Python. A code snipped in
Matlab showing how to train SVM classifier with a user-defined kernel is here:
[http://cmp.felk.cvut.cz/cmp/courses/SSU/svm_dns/demo_libsvm.m](http://cmp.felk.cvut.cz/cmp/courses/SSU/svm_dns/demo_libsvm.m)

## 3 String Sub-sequence Kernel

The formal definition of the SSK given in this section is adopted from the original paper [3].
The length of a strings=s 1 ,...,s|s| is denoted by|s|. The concatenation of two stringss
andtis denoted byst. Thes[i:j] is a substringsi,...,sj of the strings. The stringuis a
subsequence ofs, if there exist indicesi= (i 1 ,...,i|u|), with 1≤i 1 <···< i|u|≤|s|, such that
uj =sij, forj = 1,...,|u|, oru=s[i] for short. The lengthl(i) of subsequenceu=s[i] is
l(i) =i|u|−i 1 + 1.


```
The input strings from Σ∗are embedded to a feature space viaφ: Σ∗→R|Σ|
q
```
. Each feature
ofφ(s) = (φu(s)|u∈Σq) is associated with one subsequence of lengthq. The feature value is
defined as
φu(s) =

#### ∑

```
i:s(i)=u
```
```
λl(i), u∈Σq,
```
whereλ∈(0,1] is a decay parameter. The larger the length of the subsequenceu=s(i), the
smaller contribution to the featureφu(x).
Let us consider subsequences of lengthq= 2 and strings ”cat”, ”car”, ”bat” and ”bar”. The
corresponding non-zero features are listed in the following table:

```
x φca(s) φct(s) φat(s) φba(s) φbt(s) φcr(s) φar(s) φbr(s)
”cat” λ^2 λ^3 λ^200000
”car” λ^20000 λ^3 λ^20
”bat” 0 0 λ^2 λ^2 λ^3000
”bar” 0 0 0 λ^200 λ^2 λ^3
```
For example,φca(”cat”) =λ^2 because the subsequenceu= ”ca” appears ins= ”cat”, indeed
s(i) =ufori= (1,2), and ”ca” is of the lengthl(i) = 2. Howeverφct(”cat”) =λ^3 because the
subsequenceu= ”ct” appears ins= ”cat”, that iss(i) = ”ct” fori= (1,3), but it has length
l(i) = 3 due to the skipped letter ”a” in ”cat”.
The dot product of the feature vectors for stringssandtis

```
kq(s,t) =〈φ(s),φ(t)〉=
```
#### ∑

```
u∈Σq
```
#### ∑

```
i:u=s[i]
```
```
λl(i)
```
#### ∑

```
j:u=t[j]
```
```
λl(j)=
```
#### ∑

```
u∈Σq
```
#### ∑

```
i:u=s[i]
```
#### ∑

```
j:u=t[j]
```
```
λl(i)+l(j).
```
The dot products forq= 2 and strings ”cat”, ”car”, ”bat”, ”bar” are as follows:

```
k 2 (s,t) ”cat” ”car” ”bat” ”bar”
”cat” 2 λ^4 +λ^6 λ^4 λ^40
”car” λ^42 λ^4 +λ^60 λ^4
”bat” λ^402 λ^4 +λ^6 λ^4
”bar” 0 0 λ^42 λ^4 +λ^6
```
In order to make the similarity less dependent on the length of the input string, it is recom-
mended to use theL 2 -normalized kernel

```
ˆkq(s,t) =√ kq(s,t)
kq(s,s)
```
#### √

```
kq(t,t)
```
#### .

A naive computation of the kernel value requiresO(|Σ|q) time and space since this is the
number of features involved. An efficient algorithm to evaluate the kernel uses an auxiliary
function
k′i(s,t) =

#### ∑

```
u∈Σi
```
#### ∑

```
i:u=s[i]
```
#### ∑

```
j:u=t[j]
```
```
λ|s|+|t|−i^1 −j^1 +2, i∈{ 1 ,...,q− 1 }
```
The auxiliary kernel functionk′i(s,t) differs from the original kernelki(s,t) just in the way
how it counts the length of the subsequences. In particular,ki′(s,t) counts the length from the
beginning of the sequence till the end of the strings instead of using the position of the last
symbol. For example, the auxiliary kernel counts the length of the subsequence ”ca” in the
string ”cat” to be 3 instead of 2.


```
Using the auxiliary kernel, the SSK can be computed by the following recursive procedure:
k′ 0 (s,t) = 1,for alls,t
ki′(s,t) = 0,if min{|s|,|t|}< i
ki(s,t) = 0,if min{|s|,|t|}< i
ki′(sx,t) = λk′i(s,t) +
```
#### ∑

```
j:tj=x
```
```
k′i− 1 (s,t[1,...,j−1])λ|t|−j+2, i∈{ 1 ,...,q− 1 }
```
```
kq(sx,t) = kq(s,t) +
```
#### ∑

```
j:tj=x
```
```
kq′− 1 (s,t[1,...,j−1])λ^2
```
## 4 Data

The data can be downloaded from this link
[http://cmp.felk.cvut.cz/cmp/courses/SSU/svm_dns/dns_data.zip](http://cmp.felk.cvut.cz/cmp/courses/SSU/svm_dns/dns_data.zip)
The zip file contains:

```
dns_data.mat...training, validation and test examples stored in Matlab format.
{trn,tst,val}_malware.txt...malicious examples stored in a plain text file.
{trn,tst,val}_legit.txt...contains legitimate examples stored in a plain text file.
```
The examples in the MAT file and the text files are identical. The two formats are used just
for your convenience (Matlab vs. Python programmers). The examples are split randomly into
three parts: 1000 training, 500 validation and 2000 test examples. The training examples serve
for learning the parametersαandbof the kernel SVM classifier (5). The validation examples
should be used for selection of the best value of the hyper-parameterC. The test examples are
intended for the final evaluation of the SVM classifier with the bestC.
For those students who do not manage to implement the SSK kernel we provide precomputed
kernel matrices:

```
Ktrn[1000×1000] dot products between training examples.
Kval[500×1000] dot products between validation and training examples.
Ktst[2000×1000] dot products between test and training examples.
```
The kernel values were computed using theL 2 -normalization,q= 2 andλ= 0.4. The kernel
matrices can be downloaded from this link
[http://cmp.felk.cvut.cz/cmp/courses/SSU/svm_dns/dns_data_kernel.zip](http://cmp.felk.cvut.cz/cmp/courses/SSU/svm_dns/dns_data_kernel.zip)
The zip file contains:

```
dns_data_kernel.mat...MAT file containing all kernel matrices together with the labels.
{trn,val,tst}_kernel_mat.svmlight...kernel matrices with corresponding labels stored
in SVMlightformat. The SVMlightformat is plain text where each line starts with the
label (+1or−1) followed by a list of indices and values of the non-zero elements of the
corresponding row of the kernel matrix:
```
```
<label> <index1>:<value1> <index2>:<value2> ...
```
```
Hence the file has as many lines as the number of rows of the corresponding kernel matrix.
The Matlab interface of the libSVM provides a function to load the labels and the kernel
matrix by
```
```
[labels, kernel_matrix] = libsvmread(’{trn,val,tst}_kernel_mat.svmlight’)
```

## 5 Task assignment

Assignment 1 (6 points) Train the kernel SVM classifier for a set of regularization constants
C∈ { 0. 01 , 0. 1 , 1 , 10 , 100 }. Use theL 2 -normalized SSK kernel withq= 2andλ= 0. 4. Create
a table which will contain the training error, the validation error and the number of support
vectors for each C ∈ { 0. 01 , 0. 1 , 1 , 10 , 100 }. In addition, display the training errors and the
validation error as a function ofCin a graph (use logarithmic x-axis). By errors we mean the
empirical risk with 0 / 1 -loss estimated on the corresponding part of the examples.

Remark: You are allowed to use the pre-computed kernel matrices.

Assignment 2 (4 points) Select the best regularization constantCbased on the minimal val-
idation error. Report the best constantCand the test errorRSl(h)of the corresponding SVM
classifier. Compute the minimal value of εsuch that the true classification error is in the
interval(RSl(h)−ε,RSl(h) +ε)with the probability99%at least.

Assignment 3 (5 bonus points) Implement a function which computes the SSK kernel. For
sequence lengthq = 3(note it is different length than in Assignment 1) and the decay pa-
rameterλ= 0. 4 , compute a kernel matrix describing similarity between the following strings:
“google.com”, “facebook.com”, “atqgkfauhuaufm.com” and “vopydum.com”. Repeat the compu-
tation forL 2 -normalized SSK. Report the obtained kernel matrices in the document (in total 2
matrices of sizeR^4 ×^4 ). The implemented function should have the following syntax:

```
k = subseq_kernel( str1, str2, q, lambda)
```
Remark: Assignment 3 is not obligatory to gain full number of points from this lab (that is 10).
However, if you solve it you will gain 5 bonus points.

Hint: It is recommended not to use recursive functions in Matlab. Rather implement the recur-
rent formulas using embedded loops.

## References

[1] Chang Chih-Chung and Lin Chih-Jen. Libsvm: [http://www.csie.ntu.edu.tw/~cjlin/](http://www.csie.ntu.edu.tw/~cjlin/)
libsvm/.

[2] Haddadi F. and Zincir-Heywood A.N. Analyzing string format-based classifiers for botnet
detection: GP and SVM. InIEEE Congress on evolutionary Computation, 2013.

[3] Lodhi H., Saunders C., Shawe-Taylor J., Cristianini N., and Watkins C. Text classification
using string kernels.Journal of Machine Learning Research, 2002.


